[kaggle 데이터 활용](https://www.kaggle.com/mboaglio/simplifiedhuarus)

xgboost는 결정 트리 기반 앙상블한 모델입니다.
결정트리 앙상블 학습법의 배깅과 부스팅 중 배깅으로 진행한 것이 랜덤포레스트, 부스팅으로 진행한 것이 AdaBoost(에이다부스트), Gradient Boosting (그래디언트 부스팅), XGBoost, LightGBM, CatBoost 등등 
xgboost는 Gradient Boosting의 확장으로, 빠른 학습과 예측 속도, 높은 성능을 제공하는 알고리즘입니다. 즉, 랜덤포레스트는 개별 결정 트리가 독립적인 값 중 최적의 값을 찾는다면 부스팅 방식은 이전 모델이 만든 잔차(오차)에 새로운 모델을 학습시켜 오차를 보정해 나가는 방식. 즉 성능은 더 높으나 과적합 문제가 있을 수 있고 느림. 근데 gradient boosting에서 과적합 문제를 좀 더 해결한 것이 xgboost라고 본 것 같음.

xgboost 지원하는 feature importance 타입 1) weight 2) gain, total gain 3) cover, total cover
gain 은 feature가 사용된 전체 노드의 평균 gain
total gain은                   ''                     gain의 총합

pip install eli5
바이러스 에러 -> window 보안 실시간 잠깐 꺼주고 설치

permutation importance 는 feature 하나하나마다 shuffle을 하며 성능 변화를 지켜보고,
만약 그 feature가 모델링에서 중요한 역할을 하고 있었다면 성능이 크게 떨어질 것이라는 개념으로 출발합니다.

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import SelectFromModel

train = pd.read_csv('Module36/train.csv')
X_test = pd.read_csv('Module36/test.csv')
train.sample(5)

print(train['activity'].unique())
label_encoder = LabelEncoder()
train['activity'] = label_encoder.fit_transform(train['activity'])
print(train['activity'])

# label, target
# 2 : standing, 1 : sitting, 0 : laying, 3 : walking, 4 : walking_downstairs, 5 : walking_upstairs
labels = train['activity']
print(f"labels : {labels.unique()}")
train_data = train.drop(['rn', 'activity'], axis = 1)

X_train, X_val, y_train, y_val = train_test_split(train, labels, test_size=0.2, random_state=42)

xgb_model = XGBClassifier()
xgb_model.fit(X_train, y_train)

# feature importance - permutation importance
import eli5 # 바이러스~~ 작업 완료 x 에러 시 windows 보안 실시간 해제 후 설치
from eli5.sklearn import PermutationImportance
from sklearn.ensemble import RandomForestClassifier
# 미리 사용할 알고리즘을 fit 해야한다
model = RandomForestClassifier().fit(X_train, y_train)

perm = PermutationImportance(model, scoring = "accuracy", random_state = 22).fit(X_val, y_val) 
eli5.show_weights(perm, top = 30, feature_names = X_val.columns.tolist())

minimum_importance = 0.000
mask = perm.feature_importances_ > minimum_importance
features = X_train.columns[mask]
X_train_selected = X_train[features]
X_val_selected = X_val[features]

# 선택된 중요 특성을 사용하여 XGBoost 분류기 훈련
xgb_model_selected = XGBClassifier()
xgb_model_selected.fit(X_train_selected, y_train)

# 검증 세트에서 예측 수행
y_val_pred = xgb_model_selected.predict(X_val_selected)

# 검증 세트에서 정확도 계산
accuracy_val = accuracy_score(y_val, y_val_pred)
print(f"검증 세트 정확도: {accuracy_val:.2f}")

# 테스트 데이터 준비
test_data = X_test.drop(['rn'], axis=1)

# 훈련/검증 세트와 동일한 특성 선택
X_test_selected = test_data[features]

# 테스트 세트에서 예측 수행
y_test_pred = xgb_model_selected.predict(X_test_selected)

activity_mapping = {2: 'standing', 1: 'sitting', 0: 'laying', 3: 'walking', 4: 'walking_downstairs', 5: 'walking_upstairs'}
y_test_pred_labels = [activity_mapping[pred] for pred in y_test_pred]

# 변환된 예측 결과를 CSV 파일로 저장 (필요시)
output = pd.DataFrame({'rn': X_test['rn'], 'activity': y_test_pred_labels})
output.to_csv('predictions_with_labels.csv', index=False)

# 예측 결과 출력
print("변환된 예측 결과:")
print(output['activity'].value_counts())
```