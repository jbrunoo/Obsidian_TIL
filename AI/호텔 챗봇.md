
```python
import nltk # 텍스트 데이터를 처리
import numpy as np # 말뭉치를 배열로 표현
import random 
import operator
import string # 표준 파이썬 문자열을 처리
import re
from sklearn.metrics.pairwise import cosine_similarity # 이를 나중에 사용하여 두 개의 문장이 얼마나 비슷한지를 결정합니다.
from sklearn.feature_extraction.text import TfidfVectorizer # Experience 2에서 단어 가방을 만드는 함수를 만들었던 것을 기억하십니까? 이 함수는 같은 일을 합니다!

# 1.[Dataset] Module27 (ans).txt 파일을 읽어 raw_data_ans 에 저장한다.
# 2.[Dataset] Module27 (ques).txt 파일을 읽어 raw_data_ques 에 저장한다. 
import os

ans_filepath='[Dataset] Module27 (ans).txt'
ques_filepath = '[Dataset] Module27(ques).txt'
corpus1 = open(ans_filepath, 'r', errors = 'ignore')
corpus2 = open(ques_filepath,'r', errors = 'ignore')
raw_data_ans = corpus1.read()
raw_data_ques = corpus2.read()
# 질문 및 대답 파일 내용을 문자열로 읽어 소문자로 변경하여 저장하기 
raw_data_ans = raw_data_ans.lower()
raw_data_ques = raw_data_ques.lower()
raw_data_ans_1 = re.sub(r'(\d+)\.', r'\1 .', raw_data_ans)
# 숫자 + 마침표를 문장으로 인지못해서 중간에 공백을 추가해주는 구문
# 이렇게 하지말고 lower 전에 sent_tokenize를 하고 for문으로 lower 해주는게 나을 듯

print(raw_data_ans_1)
# 저장된 데이터의 길이 확인
print(len(raw_data_ans), len(raw_data_ques))
```

```python
# 1.2.3 문장 세분화 후 길이확인하기
# question 과 answer 데이터에 대해 sent_tokenize() 사용하여 문서를 문장 목록으로 변환한다 
# sent_tokens_ques / sent_tokens_ans  에 저장
# your code here
sent_tokens_ans = nltk.sent_tokenize(raw_data_ans_1)
sent_tokens_ques = nltk.sent_tokenize(raw_data_ques)
# 토근화된 데이터 길이 확인하기 : sent_tokens_ans, sent_tokens_ques
print(len(sent_tokens_ans), len(sent_tokens_ques))
print(sent_tokens_ans)

# 질문과 답변 매칭한 딕셔너리 - ques_ans_pairs생성한 후 상위 5개 출력해 보기
# your code here
sentence_dict = {}
for ans, ques in zip(sent_tokens_ans, sent_tokens_ques):
    sentence_dict[ans] = ques

print(list(sentence_dict.items())[:5])

# raw_data_ques 와  raw_data_ans 단어 토큰화하고 출력하기 : word_tokens_ques 저장
# your code here
word_tokens_ans = nltk.word_tokenize(raw_data_ans)
word_tokens_ques = nltk.word_tokenize(raw_data_ques)

print(word_tokens_ans, '\n'*3, word_tokens_ques)

# 표제어 추출, LemTokens() 정의하기 - 이전 예제 참조
lemmer = nltk.stem.WordNetLemmatizer()
def LemTokens(tokens):
    return [lemmer.lemmatize(token) for token in tokens]


# 정규화
remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)
# LemNormalize() 정의하기 - 이전 예제 참조
def LemNormalize(text):
    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))
```

```python
# 챗봇 기능 추가 - 코사인 유사성
# 2.1.1 입력 및 응답 목록 작성
GREETING_INPUTS = ["hello", "hi", "greetings", "sup", "what's up","hey", "hey there"]
GREETING_RESPONSES = ["hi", "hey", "*nods*", "hi there", "hello", "I am glad! You are talking to me"]

# 인사말을 수신하고 반환하는 함수 만들기
def greeting(sentence):
    for word in sentence.split(): # 문장의 각 단어를 살펴봅니다.
        if word.lower() in GREETING_INPUTS: # 단어가 GREETING_INPUT와 일치하는지 확인합니다.
            return random.choice(GREETING_RESPONSES) # Greeting_Response로 답장합니다.

# # 2.1.2 끝인삿말 대응하기
GOODBYE_INPUTS = ["bye", "see you!", "unit", "exit"]
GOODBYE_RESPONSES = ["Goodbye!", "See you later!", "Take care!", "Farewell!"]

# 끝인삿말을 수신하고 반환하는 함수 만들기
def goodbye(sentence):
    for word in sentence.split():
        if word.lower() in GOODBYE_INPUTS:
            return random.choice(GOODBYE_RESPONSES)
    return None


# 2.1.3 질문을 받고 답변을 반환하는 함수 만들기
# your code here
def response(user_response):
    
    robo_response='' # 문자열을 포함하도록 변수를 초기화
    sent_tokens.append(user_response) # sent_messages에 사용자 응답 추가
    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english') 
    tfidf = TfidfVec.fit_transform(sent_tokens) # tfidf 값 가져오기
    vals = cosine_similarity(tfidf[-1], tfidf) # 코사인 유사성 값 가져오기
    idx=vals.argsort()[0][-2] 
    flat = vals.flatten() 
    flat.sort() # 오름차순으로 정렬
    req_tfidf = flat[-2] 
    
    if(req_tfidf==0):
        robo_response=robo_response+"I am sorry! I don't understand you"
        return robo_response
    else:
        robo_response = robo_response+sent_tokens[idx]
        return robo_response
```

```python
import datetime

def tell_time(sentence):
    for word in sentence.split():
        if word.lower() == 'time':
            currentdt = datetime.datetime.now()
            return currentdt.strftime("%Y-%m-%d %H:%M:%S")
        
tell_time('time')
```

```python
# 2.1.4 챗봇 테스트
# your code here
flag=True
print("ROBO: My name is Jane. I will answer your queries about this hotel. If you want to exit, type Bye!")
while(flag==True):
    user_response = input()
    user_response=user_response.lower()
    if(user_response!='bye'):
        if(user_response=='thanks' or user_response=='thank you' ):
            flag=False
            print("ROBO: You are welcome..")
        else:
            if(greeting(user_response)!=None):
                print("ROBO: "+greeting(user_response))
            if(tell_time(user_response)!=None):
                print("ROBO: "+tell_time(user_response))
            else:
                print("ROBO: ",end="")
                print(response(user_response))
                sent_tokens.remove(user_response)
    else:
        flag=False
        print("ROBO: Bye! take care..")
```


# Doc2vec 이용한 챗봇 기능
Doc2vec은 기본적으로 문서에서 벡터를 생성하는 신경망 기반 모델입니다.
#### word2vec란?[¶](http://localhost:8888/notebooks/Documents/Python_%EC%8B%A4%EC%8A%B5%ED%8C%8C%EC%9D%BC/Python_%EC%8B%A4%EC%8A%B5%ED%8C%8C%EC%9D%BC/AI4FW027/AI4FW027/%5BNotebook%20-%20Student%5D%20Module%2027%20(Hotel%20Chatbot).ipynb#word2vec%EB%9E%80?)

이는 삽입 단어를 생성하는 모델이며, 여기서는 텍스트의 큰 말뭉치를 입력으로 받고 일반적으로 수백 개 차원의 벡터 공간을 생성합니다.

